<!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="Python,神经网络,机器学习,优化,ReLU,"><link rel="alternate" href="/atom.xml" title="Tech. Design. Life." type="application/atom+xml"><meta name="description" content="So the LORD God formed the man from the dust of the ground, breathed life into his lungs, and the man became a living being.本文主要介绍笔者在进行数学建模时使用的神经网络和机器学习的部分内容。"><meta name="keywords" content="Python,神经网络,机器学习,优化,ReLU"><meta property="og:type" content="article"><meta property="og:title" content="人工神经网络"><meta property="og:url" content="http://blog.superyoung.win/2017/04/26/learning_data_analysis_with_python/about_ann/index.html"><meta property="og:site_name" content="Tech. Design. Life."><meta property="og:description" content="So the LORD God formed the man from the dust of the ground, breathed life into his lungs, and the man became a living being.本文主要介绍笔者在进行数学建模时使用的神经网络和机器学习的部分内容。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://blog.superyoung.win/images/data_analysis/Ncell.png"><meta property="og:image" content="http://blog.superyoung.win/images/data_analysis/ann.png"><meta property="og:image" content="http://blog.superyoung.win/images/data_analysis/math_expression.png"><meta property="og:image" content="http://blog.superyoung.win/images/data_analysis/math_expression_hyperplane.png"><meta property="og:image" content="http://blog.superyoung.win/images/data_analysis/sigmoid_function.png"><meta property="og:image" content="http://blog.superyoung.win/images/data_analysis/Rectifier_and_softplus_functions.png"><meta property="og:updated_time" content="2018-09-02T13:20:41.244Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="人工神经网络"><meta name="twitter:description" content="So the LORD God formed the man from the dust of the ground, breathed life into his lungs, and the man became a living being.本文主要介绍笔者在进行数学建模时使用的神经网络和机器学习的部分内容。"><meta name="twitter:image" content="http://blog.superyoung.win/images/data_analysis/Ncell.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://blog.superyoung.win/2017/04/26/learning_data_analysis_with_python/about_ann/"><title>人工神经网络 | Tech. Design. Life.</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Tech. Design. Life.</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://blog.superyoung.win/2017/04/26/learning_data_analysis_with_python/about_ann/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Soleil Yu"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Tech. Design. Life."></span><header class="post-header"><h1 class="post-title" itemprop="name headline">人工神经网络</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-26T22:09:27+08:00">2017-04-26</time></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>So the LORD God formed the man from the dust of the ground, breathed life into his lungs, and the man became a living being.</p></blockquote><p>本文主要介绍笔者在进行数学建模时使用的神经网络和机器学习的部分内容。</p><a id="more"></a><h1 id="0x00-关于人工神经网络和人工神经元"><a href="#0x00-关于人工神经网络和人工神经元" class="headerlink" title="0x00 关于人工神经网络和人工神经元"></a>0x00 关于人工神经网络和人工神经元</h1><p>在机器学习和认知科学领域，人工神经网络（英文：artificial neural network，缩写ANN），简称神经网络（英文：neural network，缩写NN）或类神经网络，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，可以用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。现代神经网络是一种非线性统计性数据建模工具。</p><p><img src="/images/data_analysis/Ncell.png" alt="人工神经元示意图"></p><p>上图是一个人工神经元的示意图，而一个人工神经网络就是许多人工神经元进行分层地互相连接，像这样：<br><img src="/images/data_analysis/ann.png" alt="人工神经网络示意图"></p><p>人工神经元的数学表示可以是这样：<br><img src="/images/data_analysis/math_expression.png" alt="人工神经元的数学表达式"></p><blockquote><p>其中 <code>W&#39;</code> 是权重向量，<code>A</code> 是输入向量，<code>b</code> 是偏置值，<code>f</code> 是激活函数</p></blockquote><p>可以看到，一个神经元的功能是求得输入向量与权向量的内积后，经一个非线性激活函数得到一个标量结果。从理论上一个三层的神经网络可以拟合任意函数。</p><p>还可以这样看神经元的作用：把一个n维的向量空间用一个超平面分成两部分（称之为判断边界），给定一个输入向量，神经元可以判断出这个向量位于超平面的哪一边。<br>那么它的数学表达式就可以写成这样：<br><img src="/images/data_analysis/math_expression_hyperplane.png" alt="人工神经元的超平面表示"></p><blockquote><p>其中，<code>W</code> 是权重向量的转置，<code>b</code> 是偏置值，<code>p</code> 是超平面向量</p></blockquote><p>既可以从数学的角度看待人工神经网络，也可以从仿生学的角度看待人工神经网络。从数学角度看，人工神经网络就是一个很好用的非线性拟合工具，除了结构上的特点以外没有什么更特别的地方，而从仿生学角度看待的话，人工神经网络是一种能让计算机逼近人类智能的工具。本文可能将会将两者结合着来看待人工神经网络，看看能不能有一些新的感悟或者发现。</p><h1 id="0x01-典型的神经网络具有以下三个主要组成部分"><a href="#0x01-典型的神经网络具有以下三个主要组成部分" class="headerlink" title="0x01 典型的神经网络具有以下三个主要组成部分"></a>0x01 典型的神经网络具有以下三个主要组成部分</h1><ul><li><strong>结构（Architecture）</strong> 结构指定了网络中的变量和它们的拓扑关系。例如，神经网络中的变量可以是神经元连接的权重（weights）和神经元的激励值（activities of the neurons）。</li><li><strong>激活函数（Activity Rule）</strong> 大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。一般激活函数依赖于网络中的权重（即该网络的参数）。</li><li><strong>学习规则（Learning Rule）</strong> 学习规则指定了网络中的权重如何随着时间推进而调整。这一般被看做是一种长时间的动力学规则。一般情况下，学习规则依赖于神经元的激励值。它也可能依赖于监督者提供的目标值和当前权重的值。现在较常用的就是误差反向传播方法。而误差的计算里面，损失函数的确定又是必不可少的。</li></ul><p>接下来就每一个部分进行详细介绍</p><h1 id="0x02-关于结构"><a href="#0x02-关于结构" class="headerlink" title="0x02 关于结构"></a>0x02 关于结构</h1><p>一种常见的结构：多层前馈网络（Multilayer Feedforward Network）由三部分组成，</p><ul><li><strong>输入层（Input layer）</strong> ，输入层的神经元（Neuron）接受输入值，称为输入向量。</li><li><strong>输出层（Output layer）</strong> ，输入向量在神经网络中传输、分析、权衡，形成输出结果。输出的消息称为输出向量。</li><li><strong>隐藏层（Hidden layer）</strong> ，或者称为“隐含层”，是输入层和输出层之间多个神经元组成的层次。隐层可以有很多层。隐层的节点（神经元）数目不定，但数目越多神经网络的非线性越显著，从而神经网络的鲁棒性（robustness）（控制系统在一定结构、大小等的参数摄动下，维持某些性能的特性。）更显著。<br>神经网络的类型已经演变出很多种，这种分层的结构也并不是对所有的神经网络都适用。<br>结构方面主要注意层数和各层节点数即可，数值的确定有一些经验法则可以用，但是往往效果并不是很好，具体数值还是要依靠多次的试验来确定。</li></ul><h1 id="0x03-关于激活函数"><a href="#0x03-关于激活函数" class="headerlink" title="0x03 关于激活函数"></a>0x03 关于激活函数</h1><p>激活函数可以理解为就是在每个人工神经元外包裹的那层函数。为什么要有激活函数存在呢？因为 <code>W*x + b</code> 仍然是线性的，再用一个线性或者非线性的函数做一次变换往往可以更好地拟合非线性的目标函数。激活函数在需要的时候可以自定义，当然常用的激活函数也有很多，这里主要介绍一下最近用过的几个常用激活函数：</p><ol><li><p>logisitic sigmoid function<br><img src="/images/data_analysis/sigmoid_function.png" alt="logisitic sigmoid function"><br>其定义域在(0,1)之间。</p></li><li><p>softmax function<br>是 sigmoid 函数的一般形式，它能将一个含任意实数的K维的向量 <strong>Z</strong> “压缩”到另一个K维实向量 <strong>σ(Z)</strong> ，使得每一个元素的范围都在(0, 1)之间，并且所有元素的和为1。</p></li><li><p>tanh function<br>tanh 全称双曲正切函数 hyperbolic tangent，可以看做是 logisitic sigmoid 函数在 y 取值为 (-1, 1) 上的版本<br>其函数表达式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tanh(x)=2⋅σ(2x)−1</span><br><span class="line">其中</span><br><span class="line">σ(x)=ex/(1+ex)</span><br></pre></td></tr></table></figure></li><li><p>Softplus &amp; ReLU function<br>Softplus 是 sigmoid 函数的原函数，是由 Charles Dugas 等人在2001年提出来的，其函数数学表达式：Softplus(x)=log(1+e^x)<br>而 ReLU 的全称是线性整流函数 Rectified Linear Unit (ReLU)，是一个定义在(0, 1)之间的函数，其在 <code>x&lt;0</code> 部分取值为0，而 <code>x&gt;0</code> 的部分为线性函数，他们的图像如下所示：<br><img src="/images/data_analysis/Rectifier_and_softplus_functions.png" alt="Softplus_ReLU"></p></li></ol><p>这里主要讲讲 ReLU 函数，ReLU 函数对于训练深层神经网络很有优势，它相比于 tanh 和 logisitic sigmoid 等的优点在于：</p><ol><li>单侧激活性：当 <code>x&lt;0</code> 时神经元不会被激活，这和真实的神经元表现相近，也因为这个，使用了 ReLU 做激活函数的神经网络的神经元激活率比较低，相比于 sigmoid 系的激活函数 ReLU 和真实的神经元更相似。</li><li>更有效的梯度传递：不会有梯度爆炸和梯度消失的问题</li><li>计算效率高：激活部分是线性函数</li></ol><p>ReLU 函数还有一些优化版本：</p><ol><li>一种是 leaky ReLU：就是当 <code>x&lt;0</code> 时 y 不是取 0，而是取一个负的线性函数的值。</li><li>另一种是 random leaky ReLU：当 <code>x&lt;0</code> 时 y 不是取 0，和上面的一样取一个负的线性函数的值，再加上一个满足正态分布的随机项。</li></ol><p>以上两种优化版的 ReLU 函数在 <code>x&gt;0</code> 时和普通的 ReLU 一样都是线性函数，只是避免了在 <code>x&lt;0</code> 时信息丢失的问题。</p><h1 id="0x04-关于学习规则"><a href="#0x04-关于学习规则" class="headerlink" title="0x04 关于学习规则"></a>0x04 关于学习规则</h1><p>学习规则就是告诉神经网络需要以什么规则来进行学习。学习规则可以被认为由损失函数和优化算法来组成。<br>首先介绍一下损失函数，所谓损失函数也被称为误差函数，是一种表示预测值和实际值误差的函数，通常可以用交叉熵（常用于分类）或者均方差（常用于回归）来表示。<br>还有优化算法，是用来寻找最优解的方法，通常有梯度下降法等。<br>接下来最主要的就是误差反向传播算法，关于什么是误差反向传播可以参考这篇文章：<a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="noopener">用误差反向传播方法训练神经网络</a><br>一句话概括来说，误差的反向传播是一种可以通过优化各层权重值和偏置值从而求出损失函数最优解的方法。</p><p>OK，上面只是简简单单的介绍了一下一些最重要的知识点，关于这些知识点的更详细的介绍可以看看 Andrew Ng 在 StandFord 大学的公开课，讲得很清楚。Andrew Ng 公开课的实践部分主要是使用 Matlab 或者 Octave，而接下来的本文呢，将介绍一下 Python 上的机器学习库：Keras 和 TensorFlow。</p><h1 id="0x05-使用-Keras-训练一个人工神经网络"><a href="#0x05-使用-Keras-训练一个人工神经网络" class="headerlink" title="0x05 使用 Keras 训练一个人工神经网络"></a>0x05 使用 Keras 训练一个人工神经网络</h1><p>首先简单介绍一下 Keras。 Keras 是一个经过高层次抽象的人工神经网络框架，使用 python 编写，可以使用 TensorFlow 作为后端。它封装了一些方法来供用户调用，对于初学者来说构建网络十分方便快捷，构建网络的代码如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Nadam</span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 定义网络类型，此处为全连接型</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'my_model'</span></span><br><span class="line">activation_name = <span class="string">'ReLU'</span></span><br><span class="line">model.add(Dense(units=<span class="number">36</span>, input_dim=x_train.shape[<span class="number">1</span>], activation=activation_name))</span><br><span class="line">model.add(Dense(units=<span class="number">18</span>, activation = activation_name))</span><br><span class="line">model.add(Dense(units=<span class="number">1</span>, activation = <span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 模型编译，指定 loss 就是上面提到的损失函数，而优化算法为 Nadam</span></span><br><span class="line">model.compile(loss=<span class="string">'mean_squared_error'</span>, optimizer=Nadam())</span><br><span class="line"><span class="comment"># # 模型训练，指定输入和输出、训练次数、以及分批训练时每批的数据量大小</span></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">618</span>, batch_size=<span class="number">61940</span>)</span><br><span class="line"><span class="comment"># # 保存模型</span></span><br><span class="line">model.save(filename+<span class="string">".h5"</span>)</span><br><span class="line"><span class="comment"># # 如果后端是 tf 的话，也可以保存神经网络的 TF 计算图</span></span><br><span class="line">tf.train.write_graph(K.get_session().graph_def, path, filename+<span class="string">".pb"</span>, <span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p></p><p>只需要上面这几行代码，我们就训练出了一个神经网络，而网络的训练的具体效果，可以通过下面的函数来查看：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # 指定测试集，使用上面定义的损失函数来评估网络的效果</span></span><br><span class="line">score = model.evaluate(x_test, y_test, batch_size=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure><p></p><p>看看，核心代码只是 model.add 和 model.compile 和 model.fit 三步而已。<br>训练后的网络，通过 model.predict 就可以获取它的输出，而再用一个函数封装一下，就可以作为一个函数来使用。同时，因为一个训练好的网络已经自动提取了特征，我们可以通过输出它的权重值来分析输入和输出之间的关系，而不需要像以前一样自己多次地猜测和实验。<br>构建一个网络本来就不应该是难事，现在有 Keras 和 TensorFlow 这类工具，构建网络就更简单了。现在的难题是：数据获取和特诊提取。</p><h1 id="0x06-Python-常用的训练数据预处理工具"><a href="#0x06-Python-常用的训练数据预处理工具" class="headerlink" title="0x06 Python 常用的训练数据预处理工具"></a>0x06 Python 常用的训练数据预处理工具</h1><p><code>scikit-learn</code> 是一个基于 Numpy、Scipy 和 matplotlib 的机器学习库。其中包含了常用的数据预处理库 <code>Preprocessing</code>，例子如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing <span class="keyword">as</span> preproc</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 使用 preprocessing.train_test_spilt 来产生一组训练集和测试集</span></span><br><span class="line"><span class="comment"># 其中 data 是一个 numpy 数组</span></span><br><span class="line">X_train, X_test, y_train, y_test = preproc.train_test_split(data, labels, test_size=<span class="number">0.3</span>, random_state=<span class="number">47</span>)</span><br><span class="line">np.save(np_save + <span class="string">"X_train.npy"</span>, X_train)</span><br><span class="line">np.save(np_save + <span class="string">"X_test.npy"</span>, X_test)</span><br><span class="line">np.save(np_save + <span class="string">"y_train.npy"</span>, y_train)</span><br><span class="line">np.save(np_save + <span class="string">"y_test.npy"</span>, y_test)</span><br><span class="line">np.save(np_save + <span class="string">"X_total.npy"</span>, data)</span><br><span class="line">np.save(np_save + <span class="string">"y_total.npy"</span>, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意：此时数据不会被标准化，所以后期在使用数据的时候需要调用标准化方法</span></span><br><span class="line">x_train = np.load(np_save + <span class="string">"X_train.npy"</span>)</span><br><span class="line">y_train = np.load(np_save + <span class="string">"y_train.npy"</span>)</span><br><span class="line">x_test = np.load(np_save + <span class="string">"X_test.npy"</span>)</span><br><span class="line">y_test = np.load(np_save + <span class="string">"y_test.npy"</span>)</span><br><span class="line">x_total = np.load(np_save + <span class="string">"X_total.npy"</span>)</span><br><span class="line">y_total = np.load(np_save + <span class="string">"y_total.npy"</span>)</span><br><span class="line">scaler_x = preproc.MinMaxScaler().fit(x_total)</span><br><span class="line">x_train = scaler_x.transform(x_train)</span><br><span class="line">x_test = scaler_x.transform(x_test)</span><br><span class="line">scaler_y = preproc.MinMaxScaler().fit(y_total)</span><br><span class="line">y_train = scaler_y.transform(y_train)</span><br><span class="line">y_test = scaler_y.transform(y_test)</span><br></pre></td></tr></table></figure><p></p><p>神经网络并不是万能的，通常说来有一些和问题相关性很强的技术难点，这些需要自己克服。<br>好了，本期的介绍大致到这里，去 Kaggle 或者随便哪里找个数据集练练手吧，试一下你就知道难点在哪里了。</p><h1 id="0x07-一些强烈推荐的相关资料"><a href="#0x07-一些强烈推荐的相关资料" class="headerlink" title="0x07 一些强烈推荐的相关资料"></a>0x07 一些强烈推荐的相关资料</h1><p><a href="http://ufldl.stanford.edu/wiki/index.php/Main_Page" target="_blank" rel="noopener">Ufldl</a><br><a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/" target="_blank" rel="noopener">神经网络和深度学习</a><br><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="noopener">斯坦福大学公开课：机器学习课程</a><br><a href="https://keras.io/" target="_blank" rel="noopener">Keras Documentation</a><br><a href="http://scikit-learn.org/" target="_blank" rel="noopener">scikit-learn</a><br><a href="https://www.tensorflow.org/" target="_blank" rel="noopener">TensorFlow</a><br><a href="https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html" target="_blank" rel="noopener">Keras as a simplified interface to TensorFlow</a><br><a href="http://playground.tensorflow.org" target="_blank" rel="noopener">A Neural Network Playground for Visually Understanding ANN</a></p><h2 id="关于卷积神经网络的简单介绍"><a href="#关于卷积神经网络的简单介绍" class="headerlink" title="关于卷积神经网络的简单介绍"></a>关于卷积神经网络的简单介绍</h2><p><a href="https://www.zhihu.com/question/39022858" target="_blank" rel="noopener">知乎上的讨论</a><br><a href="http://scs.ryerson.ca/~aharley/vis/conv/" target="_blank" rel="noopener">卷积神经网络对于 MNIST 数据集的分层可视化</a></p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Python/" rel="tag"># Python</a> <a href="/tags/神经网络/" rel="tag"># 神经网络</a> <a href="/tags/机器学习/" rel="tag"># 机器学习</a> <a href="/tags/优化/" rel="tag"># 优化</a> <a href="/tags/ReLU/" rel="tag"># ReLU</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/04/09/learning_data_analysis_with_python/about_data_preprocessing/" rel="next" title="数据分析——数据预处理"><i class="fa fa-chevron-left"></i> 数据分析——数据预处理</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/05/09/learning_data_analysis_with_python/about_pandas/" rel="prev" title="Python 中的数据挖掘库 Pandas 的常用操作和容易出现误解的地方">Python 中的数据挖掘库 Pandas 的常用操作和容易出现误解的地方 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Overview</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">Soleil Yu</p><p class="site-description motion-element" itemprop="description">This blog is about iOS, Programming, Technology, Design, Life and A.I.</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">28</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><span class="site-state-item-count">121</span> <span class="site-state-item-name">tags</span></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#0x00-关于人工神经网络和人工神经元"><span class="nav-number">1.</span> <span class="nav-text">0x00 关于人工神经网络和人工神经元</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#0x01-典型的神经网络具有以下三个主要组成部分"><span class="nav-number">2.</span> <span class="nav-text">0x01 典型的神经网络具有以下三个主要组成部分</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#0x02-关于结构"><span class="nav-number">3.</span> <span class="nav-text">0x02 关于结构</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#0x03-关于激活函数"><span class="nav-number">4.</span> <span class="nav-text">0x03 关于激活函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#0x04-关于学习规则"><span class="nav-number">5.</span> <span class="nav-text">0x04 关于学习规则</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#0x05-使用-Keras-训练一个人工神经网络"><span class="nav-number">6.</span> <span class="nav-text">0x05 使用 Keras 训练一个人工神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#0x06-Python-常用的训练数据预处理工具"><span class="nav-number">7.</span> <span class="nav-text">0x06 Python 常用的训练数据预处理工具</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#0x07-一些强烈推荐的相关资料"><span class="nav-number">8.</span> <span class="nav-text">0x07 一些强烈推荐的相关资料</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#关于卷积神经网络的简单介绍"><span class="nav-number">8.1.</span> <span class="nav-text">关于卷积神经网络的简单介绍</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2016 &mdash; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Soleil YU</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script></body></html>